# -*- coding: utf-8 -*-
"""rekomendasi film

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcLNYFMMjuzRrtRpNaPaiL4cTUzFitEV

# Import Library
mengimpor library python yang digunakan untuk menganalisis dan membangun model machine learning
"""

import pandas as pd
import numpy as np
import ast
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error

import kagglehub
import ast

"""# Loading Dataset

karena saya ingin merekomendasikan film saya perlu mengimpor dataset terlebih dahulu. Saya mengambil data dari kaggle.

pertama saya memasukkan setiap excelnya ke masing masing tempatnya, kemudian beberapa variabel saya lakukan head(), describe(), dan info() untuk mengetahui nilai dan informasi yang terdapat didalamnya. saya menampilkan grafik juga untuk melihat persebarannya.
"""

path = kagglehub.dataset_download("rounakbanik/the-movies-dataset")
print("Path to dataset files:", path)

# Path to dataset files: /root/.cache/kagglehub/assets/rounakbanik/the-movies-dataset/current
movies_metadata_path = path + '/movies_metadata.csv'
credits_path = path + '/credits.csv'
keywords_path = path + '/keywords.csv'
ratings_path = path + '/ratings.csv'
links_path = path + '/links.csv'
links_small_path = path + '/links_small.csv'
ratings_small_path = path + '/ratings_small.csv'

# Load the datasets into pandas DataFrames
movies_metadata_df = pd.read_csv(movies_metadata_path, low_memory=False, nrows=10000)
credits_df = pd.read_csv(credits_path)
keywords_df = pd.read_csv(keywords_path)
ratings_df = pd.read_csv(ratings_path)
ratings_df = ratings_df[ratings_df['userId'] <= 1000]
links_df = pd.read_csv(links_path)
links_small_df = pd.read_csv(links_small_path)
ratings_small_df = pd.read_csv(ratings_small_path)

# Calculate and print the number of unique rows in each dataset
print(f"Jumlah data unik di movies_metadata.csv: {len(movies_metadata_df.drop_duplicates())}")
print(f"Jumlah data unik di credits.csv: {len(credits_df.drop_duplicates())}")
print(f"Jumlah data unik di keywords.csv: {len(keywords_df.drop_duplicates())}")
print(f"Jumlah data unik di ratings.csv: {len(ratings_df.drop_duplicates())}")
print(f"Jumlah data unik di links.csv: {len(links_df.drop_duplicates())}")
print(f"Jumlah data unik di links_small.csv: {len(links_small_df.drop_duplicates())}")
print(f"Jumlah data unik di ratings_small.csv: {len(ratings_small_df.drop_duplicates())}")

movies_metadata_df.head()

movies_metadata_df.info()

movies_metadata_df.isnull().sum()

movies_metadata_df.describe()

credits_df.head()

credits_df.info()

credits_df.isnull().sum()

credits_df.describe()

keywords_df.head()

keywords_df.info()

keywords_df.isnull().sum()

keywords_df.describe()

ratings_df.head()

ratings_df.info()

ratings_df.isnull().sum()

ratings_df.describe()

links_df.head()

links_df.info()

links_df.isnull().sum()

links_df.describe()

plt.figure(figsize=(8,5))
sns.histplot(movies_metadata_df['vote_average'].dropna(), bins=20, kde=True)
plt.title('Distribusi Vote Average Film')
plt.xlabel('Vote Average')
plt.ylabel('Jumlah Film')
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x='rating', data=ratings_df)
plt.title('Distribusi Rating Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""# Pembersihan dan Pra Pemrosesan Data

pada tahap ini saya melakukan pembersihan dataset untuk menjadikan dataset mudah diintepretasikan dan bisa dilatih. hal yang saya lakukan

1. saya melakukan penggabungan data sesuai dengan ID

2. mengecek nilai hilang dan duplikat, saya lakukan penghapusan untuk nilai yang duplikat
"""

movies_metadata_df = movies_metadata_df[['id', 'title', 'overview', 'genres']].dropna(subset=['overview'])
movies_metadata_df['id'] = movies_metadata_df['id'].astype(str).str.split('.').str[0]

credits_df['id'] = credits_df['id'].astype(str)
keywords_df['id'] = keywords_df['id'].astype(str)

movies = movies_metadata_df.merge(credits_df, on='id').merge(keywords_df, on='id')

def get_director(crew):
    try:
        crew_list = ast.literal_eval(crew)
        if isinstance(crew_list, list):
            for person in crew_list:
                if isinstance(person, dict) and person.get('job') == 'Director':
                    return person.get('name', '')
    except (ValueError, SyntaxError):
        pass
    return ''

def get_list(obj):
    try:
        obj_list = ast.literal_eval(obj)
        if isinstance(obj_list, list):
             return [item.get('name', '') for item in obj_list if isinstance(item, dict)]
    except (ValueError, SyntaxError):
        pass
    return []

movies['cast'] = movies['cast'].apply(lambda x: get_list(x)[:3])
movies['crew'] = movies['crew'].apply(get_director)
movies['keywords'] = movies['keywords'].apply(get_list)
movies['genres'] = movies['genres'].apply(get_list)

movies['soup'] = movies['overview'] + ' ' + movies['crew'] + ' ' + \
                 movies['cast'].apply(lambda x: ' '.join(x)) + ' ' + \
                 movies['genres'].apply(lambda x: ' '.join(x)) + ' ' + \
                 movies['keywords'].apply(lambda x: ' '.join(x))

movies.isnull().sum()

for col in ['cast', 'genres', 'keywords']:
    if col in movies.columns:
        movies[col] = movies[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)

print(movies.duplicated().sum())

movies.drop_duplicates(inplace=True)

ratings = ratings_df.merge(links_df, on='movieId').dropna(subset=['tmdbId'])
ratings['tmdbId'] = ratings['tmdbId'].astype(int).astype(str)

ratings = ratings.groupby(['userId', 'tmdbId'])['rating'].mean().reset_index()

R_df = ratings.pivot(index='userId', columns='tmdbId', values='rating').fillna(0)
R = R_df.values
user_ratings_mean = R.mean(axis=1)
R_demeaned = R - user_ratings_mean.reshape(-1, 1)

U, sigma, Vt = svds(R_demeaned, k=50)
sigma = np.diag(sigma)
preds = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)
preds_df = pd.DataFrame(preds, columns=R_df.columns)

movie_map = movies[['id', 'title']].drop_duplicates().set_index('id')

ratings.isnull().sum()

ratings.duplicated().sum()

tfidf = TfidfVectorizer(stop_words='english', max_features=5000)
tfidf_matrix = tfidf.fit_transform(movies['soup'])

"""# Membangun model

tahap ini melakukan pembobotan dengan teknik TF-IDF.

saya menggunakan 2 model :`

- Content-Based Filtering
- Collaborative Filtering

collaborative akan menampilkan RMSE sedangkan CBF menampilkan precision, recall, dan f1-score.
"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()

def content_recommend(title, cosine_sim=cosine_sim):
    if title not in indices:
        return ["Judul tidak ditemukan"]
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]
    movie_indices = [i[0] for i in sim_scores]
    return movies['title'].iloc[movie_indices].tolist()

print("\nTop 10 Rekomendasi Film Berdasarkan Konten untuk 'Toy Story':")
print(content_recommend('Toy Story'))

print("\nTop 10 Rekomendasi Film Berdasarkan Konten untuk 'The Dark Knight Rises':")
print(content_recommend('The Dark Knight Rises'))

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_content_based(recommendations, actual_movies, k=10):

    top_k_recommendations = recommendations[:k]

    recommended_set = set(top_k_recommendations)
    actual_set = set(actual_movies)

    true_positives = len(recommended_set.intersection(actual_set))

    precision = true_positives / k if k > 0 else 0

    if not actual_set:
      recall = 1.0 if true_positives == 0 else 0.0
    else:
      recall = true_positives / len(actual_set)

    if precision + recall == 0:
        f1 = 0
    else:
        f1 = 2 * (precision * recall) / (precision + recall)

    return precision, recall, f1

recommended_movies_for_toy_story = content_recommend('Toy Story')

toy_story_genres = movies[movies['title'] == 'Toy Story']['genres'].iloc[0]
if isinstance(toy_story_genres, tuple):
    toy_story_genres = list(toy_story_genres)

actual_relevant_movies = movies[movies['genres'].apply(lambda genres_list: any(genre in genres_list for genre in toy_story_genres))]

actual_relevant_movie_titles = actual_relevant_movies['title'].tolist()

precision_ts, recall_ts, f1_ts = evaluate_content_based(recommended_movies_for_toy_story, actual_relevant_movie_titles)

print(f"\nEvaluasi Rekomendasi Content-Based untuk 'Toy Story':")
print(f"  Precision: {precision_ts:.4f}")
print(f"  Recall: {recall_ts:.4f}")
print(f"  F1-score: {f1_ts:.4f}")

def collab_recommend(user_id, preds_df=preds_df, movies=movies, original_ratings=R_df):
    user_row = preds_df.iloc[user_id - 1]
    already_rated = original_ratings.iloc[user_id - 1]
    unrated = user_row[already_rated == 0].sort_values(ascending=False)
    top_movies = unrated.head(10).index
    return [movie_map.loc[mid]['title'] if mid in movie_map.index else 'Unknown' for mid in top_movies]

print("\nTop 10 Rekomendasi Film Berdasarkan Collaborative Filtering untuk Pengguna 1:")
print(collab_recommend(1))

print("\nTop 10 Rekomendasi Film Berdasarkan Collaborative Filtering untuk Pengguna 10:")
print(collab_recommend(10))

mask = np.random.rand(len(ratings)) < 0.8
train_ratings = ratings[mask]
test_ratings = ratings[~mask]

test_df = test_ratings.copy()
user_id_map = {uid: idx for idx, uid in enumerate(R_df.index)}
test_df['pred'] = test_df.apply(
    lambda row: preds_df.loc[user_id_map.get(row['userId'], 0)].get(str(int(row['tmdbId'])), np.nan), axis=1)

rmse = np.sqrt(mean_squared_error(test_df['rating'], test_df['pred']))
print(f"RMSE Collaborative Filtering: {rmse:.4f}")

plt.figure(figsize=(8, 5))
sns.histplot(ratings['rating'], bins=10, kde=True)
plt.title('Distribusi Rating Pengguna')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.show()

from IPython.display import display
import ipywidgets as widgets

def show_content_based():
    input_film = widgets.Text(value='The Avengers', placeholder='Masukkan judul film', description='Film:')
    button = widgets.Button(description="Rekomendasikan")
    output = widgets.Output()

    def on_button_clicked(b):
        with output:
            output.clear_output()
            hasil = content_recommend(input_film.value)
            print("Rekomendasi berdasarkan film:", input_film.value)
            for i, film in enumerate(hasil, 1):
                print(f"{i}. {film}")

    button.on_click(on_button_clicked)
    display(input_film, button, output)

def show_collab_based():
    input_user = widgets.BoundedIntText(value=1, min=1, max=610, step=1, description='User ID:')
    button = widgets.Button(description="Rekomendasikan")
    output = widgets.Output()

    def on_button_clicked(b):
        with output:
            output.clear_output()
            hasil = collab_recommend(input_user.value)
            print(f"Rekomendasi untuk user {input_user.value}:")
            for i, film in enumerate(hasil, 1):
                print(f"{i}. {film}")

    button.on_click(on_button_clicked)
    display(input_user, button, output)

show_content_based()
show_collab_based()